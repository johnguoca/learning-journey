{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'uresnetv6e1statef1bl.pt', 'ff kNN.ipynb', 'uresnetv6e18statef1bl.pt', 'EDA and LSTM tf.ipynb', 'uresnetv6e53statef1.pt', 'unet_resnet_pytorch_v4.ipynb', 'uresnetv6e35statef1.pt', 'uresnetv6e46statef1.pt', 'uresnetv6e49statef1.pt', 'unet_resnet_pytorch.ipynb', 'uresnetv6e47statef1.pt', 'uresnetv6e33statef1.pt', 'train', 'uresnetv6e6statef1bl.pt', 'uresnetv6e11statef1.pt', 'uresnetv6e44statef1.pt', 'uresnetv6e13statef1bl.pt', 'uresnetv6e8statef1bl.pt', 'uresnetv6e2statef1.pt', 'depths.csv', 'uresnetv6e50statef1.pt', 'uresnetv6e54statef1.pt', 'uresnetv6e43statef1.pt', 'uresnetv6e5statef1bl.pt', 'uresnetv6e24statef1.pt', 'uresnetv6e30statef1.pt', 'uresnetv6e3statef1.pt', 'uresnetv6e3statef1bl.pt', 'uresnetv6e29statef1.pt', 'uresnetv6e1statef1.pt', 'uresnetv6e16statef1.pt', 'uresnetv6e41statef1.pt', 'uresnetv6e26statef1.pt', 'uresnetv6e18statef1.pt', 'uresnetv6e9statef1bl.pt', '.ipynb_checkpoints', 'uresnetv6e55statef1.pt', 'uresnetv6e36statef1.pt', 'uresnetv6e5statef1.pt', 'unet_resnet_v0.model', 'uresnetv6e28statef1.pt', 'resnet_unet_v0.ipynb', 'uresnetv6e48statef1.pt', 'uresnetv6e40statef1.pt', 'train.csv', 'submission.csv', 'uresnetv6e6statef1.pt', 'test', 'uresnetv6e20statef1.pt', 'uresnetv6e56statef1.pt', 'uresnetv6e8statef1.pt', 'unet_resnet_pytorch_v1.ipynb', 'uresnetv6e23statef1.pt', 'uresnetv6e37statef1.pt', 'unet_resnet_pytorch_v5.ipynb', 'EDA and LSTM.ipynb', 'uresnetv6e12statef1.pt', 'registryupload_1.csv', 'uresnetv6e22statef1.pt', 'unet_resnet_v0.csv', 'uresnetv6e10statef1.pt', 'uresnetv6e39statef1.pt', 'uresnetv6e32statef1.pt', 'uresnetv6e0statef1.pt', 'uresnetv6e15statef1.pt', 'uresnetv6e4statef1bl.pt', 'uresnetv6e12statef1bl.pt', 'kNN_rmse.csv', 'uresnetv6e27statef1bl.pt', 'uresnetv6e26statef1bl.pt', 'uresnetv6e19statef1.pt', 'uresnetv6e9statef1.pt', 'uresnetv6e27statef1.pt', 'uresnetv6e0statef1bl.pt', 'unet_resnet_pytorch_v2.ipynb', 'uresnetv6e21statef1.pt', 'unet_resnet_pytorch_v6.ipynb', 'uresnetv6e25statef1.pt', 'uresnetv6e20statef1bl.pt', 'uresnetv6e42statef1.pt', 'uresnetv6e13statef1.pt', 'uresnetv6e14statef1.pt', 'uresnetv6e2statef1bl.pt', 'uresnetv6e57statef1.pt', 'uresnetv6e21statef1bl.pt', 'uresnetv6e25statef1bl.pt', 'uresnetv6e34statef1.pt', 'uresnetv6e38statef1.pt', 'uresnetv6e52statef1.pt', 'uresnetv6e7statef1.pt', 'kNN_results.csv', 'uresnetv6e51statef1.pt', 'registryupload_2.csv', 'uresnetv6e45statef1.pt', 'uresnetv6e4statef1.pt', 'uresnetv6e17statef1.pt', 'uresnetv6e31statef1.pt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "# import seaborn as sns\n",
    "# sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "\n",
    "#from itertools import chain\n",
    "# from skimage.io import imread, imshow #, concatenate_images\n",
    "# from skimage.transform import resize\n",
    "# from skimage.morphology import label\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import time\n",
    "t_start = time.time()\n",
    "\n",
    "# print(\"Program started at:\", t_start)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "data_src = ''\n",
    "\n",
    "quick_try = False\n",
    "grayscale = True\n",
    "\n",
    "orig_image_size = (101, 101)\n",
    "image_size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network architecture created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Resnet34 encoder from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "# A same convolution conv3x3 layer\n",
    "def conv3x3(inplanes, planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "    \n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=1,\n",
    "        groups=groups,\n",
    "        stride=1)\n",
    "\n",
    "class CSE(nn.Module):\n",
    "    def __init__(self, in_ch, r):\n",
    "        super(CSE, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_ch, in_ch//r)\n",
    "        self.linear_2 = nn.Linear(in_ch//r, in_ch)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "\n",
    "        x = x.view(*(x.shape[:-2]),-1).mean(-1)\n",
    "        x = F.relu(self.linear_1(x), inplace=True)\n",
    "        x = self.linear_2(x)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = input_x * x\n",
    "\n",
    "        return x\n",
    "\n",
    "class SSE(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(SSE, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_ch, 1, kernel_size=1, stride=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        x = input_x * x\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SCSE(nn.Module):\n",
    "    def __init__(self, in_ch, r):\n",
    "        super(SCSE, self).__init__()\n",
    "\n",
    "        self.cSE = CSE(in_ch, r)\n",
    "        self.sSE = SSE(in_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cSE = self.cSE(x)\n",
    "        sSE = self.sSE(x)\n",
    "\n",
    "        x = cSE + sSE\n",
    "\n",
    "        return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out    \n",
    "\n",
    "# One residual block (for encoding layers)\n",
    "# Downsamples at the beginning if necessary\n",
    "class uBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, firstlayer=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.inplanes = inplanes\n",
    "        self.firstlayer = firstlayer\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # WHY ARE WE USING 3x3 WINDOW? OH WELL\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)    \n",
    "        self.bn2 = nn.BatchNorm2d(planes)  \n",
    "        self.conv2 = conv3x3(planes, planes)              \n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # print(x.size())\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.firstlayer == True: # This is probably not the best way to do this... fix in the future\n",
    "            x = self.maxpool(x)\n",
    "            x = self.conv2(x)\n",
    "            residual = self.conv1(x)\n",
    "        else:\n",
    "            x = self.conv1(x)            \n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        # print(x.size())\n",
    "        # print(residual.size())\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "        \n",
    "        x += residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Decoder block containing 2D transposed Convolution upsampling the features\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, outplanes, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(inplanes, planes, stride),\n",
    "            nn.BatchNorm2d(planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(planes, outplanes, stride),\n",
    "            nn.BatchNorm2d(outplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SCSE(outplanes, 2),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x, e=None):\n",
    "        x = F.upsample(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        if e is not None:\n",
    "            x = torch.cat([x,e], 1)\n",
    "        return self.block(x)    \n",
    "\n",
    "\n",
    "# Need this separate class to load weights into? We'll see\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "        \n",
    "# WIP\n",
    "class UResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(UResNet, self).__init__()\n",
    "        self.resnet = resnet34(pretrained=True)\n",
    "        self.conv1 = nn.Sequential(self.resnet.conv1,\n",
    "                                  self.resnet.bn1,\n",
    "                                  self.resnet.relu)\n",
    "        self.inplanes = 64\n",
    "        \n",
    "        # Encoding layers\n",
    "        self.layer1 = self.resnet.layer1\n",
    "        self.layer2 = self.resnet.layer2\n",
    "        self.layer3 = self.resnet.layer3\n",
    "        self.layer4 = self.resnet.layer4\n",
    "        \n",
    "        # self.avgpool = nn.AvgPool2d(7, stride=1)  # I don't think we need this in the U-net bottleneck layer\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes) # I don't think we need this in the U-net bottleneck layer\n",
    "        \n",
    "        # Center layer\n",
    "        self.center = nn.Sequential(#nn.BatchNorm2d(512),\n",
    "                                    #nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 256, kernel_size=3, padding=1, stride=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                                   )\n",
    "        \n",
    "        # Decoding layers with cat\n",
    "       \n",
    "        self.layer4e = DecoderBlock(256 + 512, 512, 64)\n",
    "        self.layer3e = DecoderBlock(64 + 256, 256, 64)\n",
    "        self.layer2e = DecoderBlock(64 + 128, 128, 64)\n",
    "        self.layer1e = DecoderBlock(64 + 64, 64, 64)\n",
    "        self.layer0e = DecoderBlock(64, 32, 64) # This one no cat actually\n",
    "        # Decoding layers without cat\n",
    "        \n",
    "        self.logit = nn.Sequential(conv3x3(320,64,1),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Conv2d(64, 1, kernel_size=1, padding=0),\n",
    "                                  # nn.Sigmoid() # Lovasz-hinge needs logit output, not sigmoid\n",
    "                                  )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.dropout2d = nn.Dropout2d()\n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_encoding_layer(self, block, planes, blocks, stride=1, firstlayer=False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion) # Is this batch norm necessary?,\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, firstlayer=firstlayer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        \n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "       \n",
    "    def forward(self, x):\n",
    "        # Pre-res layers\n",
    "        \n",
    "        x = self.conv1(x) # 128x128x1 -> 128x128x64\n",
    "        \n",
    "        # Encoding layers\n",
    "        conv2 = self.layer1(x) # 128x128x64 -> 64x64x64        \n",
    "        conv3 = self.layer2(conv2) # output: 32x32x128\n",
    "        conv4 = self.layer3(conv3) # output: 16x16x256\n",
    "        conv5 = self.layer4(conv4) # output: 8x8x512\n",
    "        f = self.center(conv5) #output: 4x4x256\n",
    "        \n",
    "        # Decoding layers\n",
    "        conv4e = self.layer4e(f, conv5) # output: 8x8x64\n",
    "        conv3e = self.layer3e(conv4e, conv4) # output: 16x16x64\n",
    "        conv2e = self.layer2e(conv3e, conv3) # output: 32x32x64\n",
    "        conv1e = self.layer1e(conv2e, conv2) # output: 64x64x64\n",
    "        conv0e = self.layer0e(conv1e) # output: 128x128x64\n",
    "        \n",
    "        # Hypercolumns\n",
    "        f = torch.cat([conv0e,\n",
    "                     F.interpolate(conv1e, scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                     F.interpolate(conv2e, scale_factor=4, mode='bilinear', align_corners=False),\n",
    "                     F.interpolate(conv3e, scale_factor=8, mode='bilinear', align_corners=False),\n",
    "                     F.interpolate(conv4e, scale_factor=16, mode='bilinear', align_corners=False)], 1)\n",
    "        # f = self.dropout2d(f)\n",
    "        \n",
    "        y = self.logit(f)\n",
    "\n",
    "        \n",
    "        return y\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    model.train()\n",
    "    return model\n",
    "\n",
    "def Uresnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = UResNet(uBasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    model.train()\n",
    "    return model.to(device)\n",
    "    \n",
    "model = Uresnet34()\n",
    "print(\"Network architecture created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code for performing augmentations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import cv2\n",
    "def do_augmentation(seqs, seq2_train, X_train, y_train):\n",
    "    # Use seq_det to build augmentation.\n",
    "\n",
    "    seq_det = seqs.to_deterministic()\n",
    "    X_train_aug = seq_det.augment_image(X_train)\n",
    "    X_train_aug = seq2_train.augment_image(X_train_aug)\n",
    "\n",
    "    y_train_aug = seq_det.augment_image(y_train)\n",
    "\n",
    "\n",
    "\n",
    "    if y_train_aug.shape != (101, 101):\n",
    "        X_train_aug = ia.imresize_single_image(X_train_aug, (101, 101), interpolation=\"linear\")\n",
    "        y_train_aug = ia.imresize_single_image(y_train_aug, (101, 101), interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "    return np.array(X_train_aug), np.array(y_train_aug)\n",
    "\n",
    "\n",
    "# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # horizontally flip\n",
    "\n",
    "    iaa.OneOf([\n",
    "        iaa.Noop(),\n",
    "        iaa.Affine(rotate=(-10, 10), translate_percent={\"x\": (-0.25, 0.25)}, mode='symmetric', cval=(0), backend=\"cv2\"),\n",
    "        iaa.Noop(),\n",
    "        iaa.CropAndPad(\n",
    "            percent=(-0.2, 0.2),\n",
    "            pad_mode=\"reflect\",\n",
    "            pad_cval=0,\n",
    "            keep_size=False\n",
    "        ),\n",
    "        # aa.Noop(),\n",
    "        # aa.PiecewiseAffine(scale=(0.01, 0.1), mode='edge', cval=(0)),\n",
    "    ])\n",
    "\n",
    "    # More as you want ...\n",
    "])\n",
    "seq_train = iaa.Sequential(\n",
    "    sometimes(iaa.Multiply((0.8, 1.2))),  # , per_channel=0.5\n",
    "    sometimes(iaa.Add((-0.2, 0.2))),  # , per_channel=0.5\n",
    "    sometimes(iaa.OneOf([\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05)),\n",
    "        iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "    ]))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGSSaltDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 file_list,\n",
    "                 is_test=False,\n",
    "                 divide=False,\n",
    "                 reflect=False,\n",
    "                 image_size=(128, 128)):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.divide = divide\n",
    "        self.reflect= reflect\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.orig_image_size = (101, 101)\n",
    "        self.padding_pixels = None\n",
    "        \n",
    "        \"\"\"\n",
    "        root_path: folder specifying files location\n",
    "        file_list: list of images IDs\n",
    "        is_test: whether train or test data is used (contains masks or not)\n",
    "        \n",
    "        divide: whether to divide by 255\n",
    "        image_size: output image size, should be divisible by 32\n",
    "        \n",
    "        orig_image_size: original images size\n",
    "        padding_pixels: placeholder for list of padding dimensions\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "\n",
    "        file_id = self.file_list[index]\n",
    "\n",
    "        # Get image path\n",
    "        image_folder = os.path.join(self.root_path, 'images')\n",
    "        image_path = os.path.join(image_folder, file_id + '.png')\n",
    "    \n",
    "        # Get mask path\n",
    "        mask_folder = os.path.join(self.root_path, 'masks')\n",
    "        mask_path = os.path.join(mask_folder, file_id + '.png')\n",
    "\n",
    "        # Load image\n",
    "        image = self.__load_image(image_path)\n",
    "        if not self.is_test:\n",
    "            # Load mask for training or evaluation\n",
    "            mask = self.__load_image(mask_path, mask=True)\n",
    "            if self.divide:\n",
    "                image = image / 255.\n",
    "                mask = mask / 255.\n",
    "            # Transform into torch float Tensors of shape (CxHxW).\n",
    "            image = torch.from_numpy(\n",
    "                np.expand_dims(image, axis=-1)).float().permute([2, 0, 1])            \n",
    "            image = self.__add_depth_channels(image)\n",
    "            mask = torch.from_numpy(\n",
    "                np.expand_dims(mask, axis=-1)).float().permute([2, 0, 1])\n",
    "            return image, mask\n",
    "\n",
    "        if self.is_test:\n",
    "            if self.divide:\n",
    "                image = image / 255.\n",
    "            image = torch.from_numpy(np.expand_dims(image, axis=-1)).float().permute([2, 0, 1])\n",
    "            image = self.__add_depth_channels(image)\n",
    "            return (image,)\n",
    "\n",
    "    def set_padding(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute padding borders for images based on original and specified image size.\n",
    "        \"\"\"\n",
    "        \n",
    "        pad_floor = np.floor(\n",
    "            (np.asarray(self.image_size) - np.asarray(self.orig_image_size)) / 2)\n",
    "        pad_ceil = np.ceil((np.asarray(self.image_size) -\n",
    "                            np.asarray(self.orig_image_size)) / 2)\n",
    "\n",
    "        self.padding_pixels = np.asarray(\n",
    "            (pad_floor[0], pad_ceil[0], pad_floor[1], pad_ceil[1])).astype(np.int32)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __pad_image(self, img):\n",
    "        \n",
    "        \"\"\"\n",
    "        Pad images according to border set in set_padding.\n",
    "        Original image is centered.\n",
    "        \"\"\"\n",
    "\n",
    "        y_min_pad, y_max_pad, x_min_pad, x_max_pad = self.padding_pixels[\n",
    "            0], self.padding_pixels[1], self.padding_pixels[2], self.padding_pixels[3]\n",
    "\n",
    "        img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad,\n",
    "                                 x_min_pad, x_max_pad,\n",
    "                                 cv2.BORDER_REFLECT)\n",
    "\n",
    "        assert img.shape[:2] == self.image_size, '\\\n",
    "        Image after padding must have the same shape as input image.'\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __load_image(self, path, mask=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Helper function for loading image.\n",
    "        If mask is loaded, it is loaded in grayscale (, 0) parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        if mask:\n",
    "            img = cv2.imread(str(path), 0)\n",
    "        else:\n",
    "            img = cv2.imread(str(path), 0)\n",
    "\n",
    "        height, width = img.shape[0], img.shape[1]        \n",
    "        \n",
    "               \n",
    "        if(self.reflect):\n",
    "            \n",
    "            img = np.fliplr(img)\n",
    "            \n",
    "                \n",
    "        img = self.__pad_image(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def return_padding_borders(self):\n",
    "        \"\"\"\n",
    "        Return padding borders to easily crop the images.\n",
    "        \"\"\"\n",
    "        return self.padding_pixels\n",
    "    \n",
    "    def __add_depth_channels(self, image):    \n",
    "        \"\"\"\n",
    "        Encodes depth information into channels.\n",
    "        \"\"\"\n",
    "        _, h, w = image.size()  \n",
    "        \n",
    "        \n",
    "\n",
    "        temp1 = torch.zeros([1, h, w], dtype=torch.float32)\n",
    "        temp2 = torch.zeros([1, h, w], dtype=torch.float32)\n",
    "\n",
    "        for row, const in enumerate(np.linspace(0, 1, h)):\n",
    "            temp1[0, row, :] = const\n",
    "        temp2 = image[0] * temp1\n",
    "        image = torch.cat((image, temp1, temp2), 0)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load initial data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training images: 4000\n",
      "# of training masks: 4000\n",
      "# of test images: 18000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rle_mask</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>575d24d81d</th>\n",
       "      <td>NaN</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a266a2a9df</th>\n",
       "      <td>5051 5151</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75efad62c1</th>\n",
       "      <td>9 93 109 94 210 94 310 95 411 95 511 96 612 96...</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34e51dba6a</th>\n",
       "      <td>48 54 149 54 251 53 353 52 455 51 557 50 659 4...</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875705fb0</th>\n",
       "      <td>1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     rle_mask    z\n",
       "id                                                                \n",
       "575d24d81d                                                NaN  843\n",
       "a266a2a9df                                          5051 5151  794\n",
       "75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...  468\n",
       "34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...  727\n",
       "4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...  797"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col = 'id')\n",
    "depths_df = pd.read_csv('depths.csv', index_col='id')\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)] # All depths not in train dataset are in \n",
    "\n",
    "print ('# of training images:', len(os.listdir('train/images')))\n",
    "print ('# of training masks:', len(os.listdir('train/masks')))\n",
    "print ('# of test images:', len(os.listdir('test/images')))\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fac60921b004ea4810870fbae38a613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "print('Loading training set.')\n",
    "for i in tqdm_notebook(train_df.index):\n",
    "    img_src = 'train/images/{}.png'.format(i)\n",
    "    mask_src = 'train/masks/{}.png'.format(i)\n",
    "    if grayscale:\n",
    "        img_temp = cv2.imread(img_src, 0)        \n",
    "    else:\n",
    "        img_temp = cv2.imread(img_src)\n",
    "    mask_temp = cv2.imread(mask_src, 0)\n",
    "    if orig_image_size != image_size:\n",
    "        img_temp = cv2.resize(img_temp, image_size)\n",
    "        mask_temp = cv2.resize(mask_temp, image_size)\n",
    "    X_train.append(img_temp)\n",
    "    y_train.append(mask_temp)\n",
    "    # print(img_temp.shape)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "if grayscale:\n",
    "    X_train = np.expand_dims(X_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute mask coverage for each observation.\n"
     ]
    }
   ],
   "source": [
    "print('Compute mask coverage for each observation.')\n",
    "\n",
    "def cov_to_class(val):\n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i:\n",
    "            return i\n",
    "\n",
    "# Percent of area covered by mask.\n",
    "train_df['coverage'] = np.mean(y_train / 255., axis=(1, 2))\n",
    "train_df['coverage_class'] = train_df.coverage.map(\n",
    "    cov_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters for data loading:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "\n",
    "train_ids = train_df.index.values\n",
    "test_ids = test_df.index.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_ids, valid_ids, tr_coverage, valid_coverage = train_test_split(\n",
    "    train_ids,\n",
    "    train_df.coverage.values,\n",
    "    test_size=0.1, stratify=train_df.coverage_class, random_state= 888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define Data Loading__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset:\n",
    "dataset_train = TGSSaltDataset(train_path, tr_ids, divide=True)\n",
    "dataset_train.set_padding()\n",
    "y_min_pad, y_max_pad, x_min_pad, x_max_pad = dataset_train.return_padding_borders()\n",
    "        \n",
    "# Validation dataset:\n",
    "dataset_val = TGSSaltDataset(train_path, valid_ids, divide=True)\n",
    "dataset_val.set_padding()\n",
    "\n",
    "# Test dataset:\n",
    "dataset_test = TGSSaltDataset(test_path, test_ids, is_test=True, divide=True)\n",
    "dataset_test.set_padding()\n",
    "\n",
    "\n",
    "# Data loaders:\n",
    "# Use multiple workers to optimize data loading speed.\n",
    "# Pin memory for quicker GPU processing.\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)\n",
    "\n",
    "# Do not shuffle for validation and test.\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lovasz Loss Function:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/bermanmaxim/LovaszSoftmax/blob/master/pytorch/lovasz_losses.py\n",
    "\"\"\"\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n",
    "    \"\"\"\n",
    "    IoU for foreground class\n",
    "    binary: 1 foreground, 0 background\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        intersection = ((label == 1) & (pred == 1)).sum()\n",
    "        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n",
    "        if not union:\n",
    "            iou = EMPTY\n",
    "        else:\n",
    "            iou = float(intersection) / union\n",
    "        ious.append(iou)\n",
    "    iou = mean(ious)    # mean accross images if per_image\n",
    "    return 100 * iou\n",
    "\n",
    "\n",
    "def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n",
    "    \"\"\"\n",
    "    Array of IoU for each (non ignored) class\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        iou = []    \n",
    "        for i in range(C):\n",
    "            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n",
    "                intersection = ((label == i) & (pred == i)).sum()\n",
    "                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n",
    "                if not union:\n",
    "                    iou.append(EMPTY)\n",
    "                else:\n",
    "                    iou.append(float(intersection) / union)\n",
    "        ious.append(iou)\n",
    "    ious = map(mean, zip(*ious)) # mean accross images if per_image\n",
    "    return 100 * np.array(ious)\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.elu_(errors_sorted) + 1, Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "class StableBCELoss(torch.nn.modules.Module):\n",
    "    def __init__(self):\n",
    "         super(StableBCELoss, self).__init__()\n",
    "    def forward(self, input, target):\n",
    "         neg_abs = - input.abs()\n",
    "         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "         return loss.mean()\n",
    "\n",
    "\n",
    "def binary_xloss(logits, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Cross entropy loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    logits, labels = flatten_binary_scores(logits, labels, ignore)\n",
    "    loss = StableBCELoss()(logits, Variable(labels.float()))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# --------------------------- MULTICLASS LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n",
    "                          for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, only_present=False):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "    \"\"\"\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    for c in range(C):\n",
    "        fg = (labels == c).float() # foreground for class c\n",
    "        if only_present and fg.sum() == 0:\n",
    "            continue\n",
    "        errors = (Variable(fg) - probas[:, c]).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return mean(losses)\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    B, C, H, W = probas.size()\n",
    "    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = (labels != ignore)\n",
    "    vprobas = probas[valid.nonzero().squeeze()]\n",
    "    vlabels = labels[valid]\n",
    "    return vprobas, vlabels\n",
    "\n",
    "def xloss(logits, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Cross entropy loss\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n",
    "\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS ---------------------------\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to load previous model\n",
    "# model = torch.load('uresnetv4e15.pt')\n",
    "model.load_state_dict(torch.load('uresnetv6e18statef1bl.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be presented as metric\n",
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric =[]\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch] > 0, B[batch] > 0\n",
    "        intersection = np.logical_and(t, p)\n",
    "        union = np.logical_or(t,p)\n",
    "        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# Set optimizer\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 0.0001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2e945bc5074fb9a0f90c40e105495e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train: 0.821, Val: 1.102, Train Accuracy: 0.805, Val Accuracy: 0.735, Best Loss: 0.290, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a5801446d44b0689c4e276c817a039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train: 0.712, Val: 1.134, Train Accuracy: 0.816, Val Accuracy: 0.729, Best Loss: 0.267, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b7dbefb18e4ea3be8feb053e45ad9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train: 0.684, Val: 1.091, Train Accuracy: 0.818, Val Accuracy: 0.741, Best Loss: 0.259, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b9118be072441abe654220d985cb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train: 0.650, Val: 1.084, Train Accuracy: 0.823, Val Accuracy: 0.746, Best Loss: 0.259, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9d83d71a4a4658af600406c18bb4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train: 0.632, Val: 1.094, Train Accuracy: 0.824, Val Accuracy: 0.740, Best Loss: 0.259, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22c17f615f342bfaaa793ddf8ff91df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train: 0.608, Val: 1.093, Train Accuracy: 0.824, Val Accuracy: 0.739, Best Loss: 0.231, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a175213dad4b46ecbca9fcb82296602a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train: 0.589, Val: 1.161, Train Accuracy: 0.824, Val Accuracy: 0.725, Best Loss: 0.231, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabf0c7ecdb346faa20793588493884c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train: 0.547, Val: 1.188, Train Accuracy: 0.831, Val Accuracy: 0.732, Best Loss: 0.231, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63497ce1eeae4fe6b893548e1a651239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train: 0.543, Val: 1.310, Train Accuracy: 0.831, Val Accuracy: 0.726, Best Loss: 0.231, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eb3c93451541f2b896372caa261afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train: 0.525, Val: 1.320, Train Accuracy: 0.836, Val Accuracy: 0.719, Best Loss: 0.230, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35bea71ada546d88238e2ba8dbb6d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train: 0.519, Val: 1.251, Train Accuracy: 0.834, Val Accuracy: 0.721, Best Loss: 0.210, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c369a765b04a369611d829d96ccb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train: 0.521, Val: 1.229, Train Accuracy: 0.832, Val Accuracy: 0.733, Best Loss: 0.210, Aug: 1.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea894ef6556483e850e2c6691302ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train: 0.476, Val: 1.257, Train Accuracy: 0.840, Val Accuracy: 0.745, Best Loss: 0.210, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc698fe4039f4474993c5adf2eb6a193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train: 0.469, Val: 1.382, Train Accuracy: 0.842, Val Accuracy: 0.729, Best Loss: 0.146, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b604cc896ee1476e851a61ae2a311302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train: 0.479, Val: 1.276, Train Accuracy: 0.841, Val Accuracy: 0.725, Best Loss: 0.146, Aug: 0.000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18db92b63f0c48639f5170e7feb94903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=113), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "from torch.autograd import Variable\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Set Binary Crossentropy as loss function.\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "lovasz = True\n",
    "\n",
    "# Train for n epochs\n",
    "n = 70\n",
    "best_loss = 1000\n",
    "\n",
    "# Random generator\n",
    "import random\n",
    "\n",
    "def reflect(x):\n",
    "    x = x.cpu().detach().numpy()\n",
    "    x_flipped = np.flip(x, 3).copy()\n",
    "    x_flipped = torch.from_numpy(x_flipped)\n",
    "    return x_flipped\n",
    "\n",
    "for e in range(n):\n",
    "    \n",
    "    # aug = 1\n",
    "    # Training:\n",
    "    train_loss = []\n",
    "    train_iou = []\n",
    "    for image, mask in tqdm_notebook(train_loader):        \n",
    "        aug = random.randint(0, 1)\n",
    "        \n",
    "        gc.collect()\n",
    "        if aug == 1:\n",
    "            image = reflect(image)\n",
    "            mask = reflect(mask)\n",
    "        \n",
    "        # Put image on chosen device\n",
    "        image = image.type(torch.float).to(device)\n",
    "        # Predict with model:\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        # Compute loss between true and predicted values\n",
    "        if lovasz:\n",
    "            loss = lovasz_hinge(y_pred, mask.to(device))\n",
    "        else:\n",
    "            loss = loss_fn(y_pred,mask.to(device))\n",
    "        iou = get_iou_vector(y_pred.cpu().detach().numpy() > 0.5, mask.numpy())\n",
    "\n",
    "        # Set model gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagate the loss.\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform single optimization step - parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append training loss\n",
    "        train_loss.append(loss.item())\n",
    "        train_iou.append(iou)\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            torch.save(model.state_dict(), 'uresnetv6e{}statef1bl.pt'.format(e))\n",
    "            best_loss = loss.item()\n",
    "\n",
    "    # Validation:\n",
    "    val_loss = []\n",
    "    val_iou = []\n",
    "    for image, mask in valid_loader:\n",
    "        \n",
    "        image = image.to(device)\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        if lovasz:\n",
    "            loss = lovasz_hinge(y_pred, mask.to(device))\n",
    "        else:\n",
    "            loss = loss_fn(y_pred,mask.to(device))\n",
    "        iou = get_iou_vector(y_pred.cpu().detach().numpy() > 0.5, mask.numpy())\n",
    "        val_loss.append(loss.item())\n",
    "        val_iou.append(iou)\n",
    "\n",
    "    print(\"Epoch: %d, Train: %.3f, Val: %.3f, Train Accuracy: %.3f, Val Accuracy: %.3f, Best Loss: %.3f, Aug: %.3f\" %\n",
    "          (e, np.mean(train_loss), np.mean(val_loss), np.mean(train_iou), np.mean(val_iou), best_loss, aug))\n",
    "    # torch.save(model, 'uresnetv5e{}lov.pt'.format(e))\n",
    "    torch.save(model.state_dict(), 'uresnetv6e{}statef1.pt'.format(e))\n",
    "# torch.save(model, 'uresnetv5.pt')\n",
    "# torch.save(model.state_dict(), 'uresnetv6state.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315e8a311c0f461d9d6faa38ea05817a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, Train: 1.638, Val: 1.533, Train Accuracy: 0.620, Val Accuracy: 0.627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4209ff6d801408994d5abb4a985b053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Train: 1.546, Val: 1.463, Train Accuracy: 0.632, Val Accuracy: 0.638\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2253cdf0d8074ba6bbb499d94894184c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, Train: 1.480, Val: 1.426, Train Accuracy: 0.647, Val Accuracy: 0.650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ebc41128f64b88868ab798ec166dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, Train: 1.477, Val: 1.415, Train Accuracy: 0.653, Val Accuracy: 0.653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741475ac9a974397949fbcca6b9b425f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, Train: 1.435, Val: 1.404, Train Accuracy: 0.655, Val Accuracy: 0.654\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601fcfc8c2e44e0786e323ab5b40a577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5, Train: 1.444, Val: 1.394, Train Accuracy: 0.653, Val Accuracy: 0.658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9774e410a0284d8ab6ee861d87392df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6, Train: 1.396, Val: 1.389, Train Accuracy: 0.663, Val Accuracy: 0.660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a132e120aa54a119571a89f8b14f106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7, Train: 1.407, Val: 1.388, Train Accuracy: 0.659, Val Accuracy: 0.656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c46cd8699da45548c71f5eff6535f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8, Train: 1.390, Val: 1.386, Train Accuracy: 0.660, Val Accuracy: 0.659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b60d72969574cb4a076ac9d3422ac94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9, Train: 1.374, Val: 1.374, Train Accuracy: 0.660, Val Accuracy: 0.659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649c16a5f4cd455280097bbcb3a14e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10, Train: 1.392, Val: 1.385, Train Accuracy: 0.666, Val Accuracy: 0.658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0e94a0cbdd4baa9467d1d5ee0d68b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-416:\n",
      "Process Process-413:\n",
      "Process Process-414:\n",
      "Process Process-415:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb5c4d01ef0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 10400) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-41915ab02663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlovasz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlovasz_hinge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# from torch.autograd import Variable\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# gc.collect()\n",
    "\n",
    "# # Set Binary Crossentropy as loss function.\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# lovasz = True\n",
    "\n",
    "# # Set optimizer.\n",
    "# learning_rate = 1e-4\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay = 0.001)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max = 100)\n",
    "\n",
    "\n",
    "# # Train for n epochs\n",
    "# n = 40\n",
    "# for e in range(n):\n",
    "\n",
    "#     # Training:\n",
    "#     train_loss = []\n",
    "#     train_iou = []\n",
    "#     for image, mask in tqdm_notebook(valid_loader):\n",
    "        \n",
    "#         gc.collect()\n",
    "        \n",
    "#         # Put image on chosen device\n",
    "#         image = image.type(torch.float).to(device)\n",
    "#         # Predict with model:\n",
    "#         y_pred = model(image)\n",
    "        \n",
    "#         # Compute loss between true and predicted values\n",
    "#         if lovasz:\n",
    "#             loss = lovasz_hinge(y_pred, mask.to(device))\n",
    "#         else:\n",
    "#             loss = loss_fn(y_pred,mask.to(device))\n",
    "#         iou = get_iou_vector(y_pred.cpu().detach().numpy() > 0.5, mask.numpy())\n",
    "\n",
    "#         # Set model gradients to zero.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Backpropagate the loss.\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Perform single optimization step - parameter update\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Append training loss\n",
    "#         train_loss.append(loss.item())\n",
    "#         train_iou.append(iou)\n",
    "\n",
    "#     # Validation:\n",
    "#     val_loss = []\n",
    "#     val_iou = []\n",
    "#     for image, mask in train_loader:\n",
    "        \n",
    "#         image = image.to(device)\n",
    "#         y_pred = model(image)\n",
    "        \n",
    "#         if lovasz:\n",
    "#             loss = lovasz_hinge(y_pred, mask.to(device))\n",
    "#         else:\n",
    "#             loss = loss_fn(y_pred,mask.to(device))\n",
    "#         iou = get_iou_vector(y_pred.cpu().detach().numpy() > 0.5, mask.numpy())\n",
    "#         val_loss.append(loss.item())\n",
    "#         val_iou.append(iou)\n",
    "\n",
    "#     print(\"Epoch: %d, Train: %.3f, Val: %.3f, Train Accuracy: %.3f, Val Accuracy: %.3f\" %\n",
    "#           (e, np.mean(train_loss), np.mean(val_loss), np.mean(train_iou), np.mean(val_iou)))\n",
    "#     # torch.save(model, 'uresnetv5e{}lov.pt'.format(e))\n",
    "#     torch.save(model.state_dict(), 'uresnetv6e{}statef2.pt'.format(e))\n",
    "# # torch.save(model, 'uresnetv5.pt')\n",
    "# # torch.save(model.state_dict(), 'uresnetv6state.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Figure out best threshold:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436056d0dac945e88261dd5a7120ce0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f148c759f28>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = np.linspace(0.3,0.7,30)\n",
    "ious = []\n",
    "for threshold in tqdm_notebook(thresholds):\n",
    "    threshold_iou = []\n",
    "    for image, mask in valid_loader:\n",
    "        image = image.to(device)\n",
    "        y_pred = model(image)\n",
    "        iou = get_iou_vector(y_pred.cpu().detach().numpy() > threshold, mask.numpy())\n",
    "        threshold_iou.append(iou)\n",
    "    ious.append(np.mean(threshold_iou))\n",
    "\n",
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cf7403c6f345f685c1afbd8e57864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(18000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "test_predictions_reflect = []\n",
    "\n",
    "for image in tqdm_notebook(test_loader):\n",
    "    image_flipped = image[0].cpu().detach().numpy()\n",
    "    image_flipped = np.flip(image_flipped, 3).copy()\n",
    "    image_flipped = torch.from_numpy(image_flipped)\n",
    "    image = image[0].type(torch.float).to(device)\n",
    "    y_pred = model(image).cpu().detach().numpy()\n",
    "    image_flipped = image_flipped.type(torch.float).to(device)\n",
    "    y_pred_flipped = model(image_flipped).cpu().detach().numpy()\n",
    "    test_predictions.append(y_pred)\n",
    "    test_predictions_reflect.append(y_pred_flipped)\n",
    "\n",
    "test_predictions_stacked = np.vstack(test_predictions)[:, 0, :, :]\n",
    "test_predictions_reflect_stacked = np.vstack(test_predictions_reflect)[:, 0, :, ::-1]\n",
    "\n",
    "test_predictions_stacked = (test_predictions_stacked + test_predictions_reflect_stacked)/2\n",
    "test_predictions_stacked = test_predictions_stacked[:, y_min_pad:-y_max_pad, x_min_pad:-x_max_pad]\n",
    "print(test_predictions_stacked.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a98107db4134d4f8daf9e6654780123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1890: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 101, 101)\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "# Original way (no TTA)\n",
    "for image in tqdm_notebook(test_loader):\n",
    "    image = image[0].type(torch.float).to(device)\n",
    "    # model.load_state_dict(torch.load('uresnetv6e2statef1.pt'))\n",
    "    y_pred1 = model(image).cpu().detach().numpy()\n",
    "    #model.load_state_dict(torch.load('uresnetv6e0statef2.pt'))\n",
    "    #y_pred2 = model(image).cpu().detach().numpy()/2\n",
    "    test_predictions.append(y_pred1)\n",
    "\n",
    "    \n",
    "test_predictions_stacked = np.vstack(test_predictions)[:, 0, :, :]\n",
    "test_predictions_stacked = test_predictions_stacked[:, y_min_pad:-y_max_pad, x_min_pad:-x_max_pad]\n",
    "\n",
    "print(test_predictions_stacked.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare submission:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df7bde2f9ad461286d80b18f66ecb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rle_encode(im):\n",
    "    pixels = im.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "# To perform RLE, predictions must be in binary integer (0/1) format.\n",
    "binary_prediction = (test_predictions_stacked > 0.465).astype(int)\n",
    "\n",
    "# RLE encoding.\n",
    "all_masks = {idx:rle_encode(binary_prediction[i])\n",
    "                           for i, idx in enumerate(\n",
    "                               tqdm_notebook(test_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict(all_masks, orient='index')\n",
    "submission.index.names = ['id']\n",
    "submission.columns = ['rle_mask']\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program started at: 1539044371.7804842\n",
      "['sample_submission.csv', 'ff kNN.ipynb', 'EDA and LSTM tf.ipynb', 'test.zip', 'unet_resnet_pytorch.ipynb', 'train', 'depths.csv', 'train.zip', '.ipynb_checkpoints', 'unet_resnet_v0.model', 'resnet_unet_v0.ipynb', 'train.csv', 'test', 'EDA and LSTM.ipynb', 'registryupload_1.csv', 'unet_resnet_v0.csv', 'kNN_rmse.csv', 'kNN_results.csv', 'registryupload_2.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "# import seaborn as sns\n",
    "# sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook #, tnrange\n",
    "\n",
    "#from itertools import chain\n",
    "# from skimage.io import imread, imshow #, concatenate_images\n",
    "# from skimage.transform import resize\n",
    "# from skimage.morphology import label\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "import time\n",
    "t_start = time.time()\n",
    "\n",
    "print(\"Program started at:\", t_start)\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network architecture created.\n"
     ]
    }
   ],
   "source": [
    "# Resnet34 encoder from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "# A same convolution conv3x3 layer\n",
    "def conv3x3(inplanes, planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    if mode == 'transpose':\n",
    "        return nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "    else:\n",
    "        # out_channels is always going to be the same\n",
    "        # as in_channels\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "            conv1x1(in_channels, out_channels))\n",
    "    \n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=1,\n",
    "        groups=groups,\n",
    "        stride=1)\n",
    "\n",
    "# One residual block (for encoding layers)\n",
    "# Downsamples at the beginning if necessary\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.inplanes = inplanes\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) # WHY ARE WE USING 3x3 WINDOW? OH WELL\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)    \n",
    "        self.bn2 = nn.BatchNorm2d(planes)  \n",
    "        self.conv2 = conv3x3(planes, planes)              \n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        if self.inplanes == 64: # This is probably not the best way to do this... fix in the future\n",
    "            out = self.maxpool(out)\n",
    "            out = self.conv2(out)\n",
    "        else:\n",
    "            out = self.conv1(x)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "                \n",
    "        out += residual\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Decoder block containing 2D transposed Convolution upsampling the features\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            conv3x3(inplanes, outplanes, stride),\n",
    "            nn.ReLU(inplace=True),\n",
    "            conv3x3(outplanes, outplanes, stride),\n",
    "            nn.ReLU(inplace=True),\n",
    "            upconv2x2(outplanes, outplanes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)    \n",
    "\n",
    "# Need this separate class to load weights into? We'll see\n",
    "# class ResNet(nn.Module):\n",
    "        \n",
    "# WIP\n",
    "class UResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(UResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False) \n",
    "        self.inplanes = 64\n",
    "        \n",
    "        # Encoding layers\n",
    "        self.layer1 = self._make_encoding_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_encoding_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_encoding_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_encoding_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        # self.avgpool = nn.AvgPool2d(7, stride=1)  # I don't think we need this in the U-net bottleneck layer\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes) # I don't think we need this in the U-net bottleneck layer\n",
    "        \n",
    "        # Decoding layers with cat\n",
    "        self.layer4i = upconv2x2(512, 512)\n",
    "        self.layer4e = DecoderBlock(256 * 3, 256)\n",
    "        self.layer3e = DecoderBlock(128 * 3, 128)\n",
    "        self.layer2e = DecoderBlock(64 * 3, 64)\n",
    "        \n",
    "        # Decoding layers without cat\n",
    "        self.dec1 = DecoderBlock(64, 32)\n",
    "        self.dec2 = DecoderBlock(32, 16)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_encoding_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion) # Is this batch norm necessary?,\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "       \n",
    "    def forward(self, x):\n",
    "        # Pre-res layers\n",
    "        x = self.conv1(x) # 128x128x1 -> 64x64x64\n",
    "                \n",
    "        # Resnet layers\n",
    "        conv2 = self.layer1(x) # 64x64x64 -> 32x32x64\n",
    "        conv3 = self.layer2(conv2) # output: 16x16x128\n",
    "        conv4 = self.layer3(conv3) # output: 8x8x256\n",
    "        conv5 = self.layer4(conv4) # output: 4x4x512\n",
    "        conv4i = self.layer4i(conv5) # output: 8x8x512\n",
    "        conv4e = self.layer4e(torch.cat([conv4i, conv4], 1)) # output: 16x16x256\n",
    "        conv3e = self.layer3e(torch.cat([conv4e, conv3], 1)) # output: 32x32x128\n",
    "        conv2e = self.layer2e(torch.cat([conv3e, conv2], 1)) # output: 64x64x64\n",
    "        dec1 = self.dec1(conv2e) # output: 128x128x32\n",
    "        \n",
    "        y = conv3x3(32, 16, 1)\n",
    "        y = nn.ReLU(inplace=True)\n",
    "        y = conv3x3(16, 16, 1)\n",
    "        y = nn.ReLU(inplace=True)\n",
    "        y = conv3x3(16, 1, 1)\n",
    "        y = F.sigmoid(y)\n",
    "\n",
    "        # Don't think we need these for U-net\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "        \n",
    "        # CENTER BLOCK\n",
    "        \n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = UResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    model.train()\n",
    "    return model.to(device)\n",
    "    \n",
    "print(\"Network architecture created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGSSaltDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_path,\n",
    "                 file_list,\n",
    "                 is_test=False,\n",
    "                 divide=False,\n",
    "                 image_size=(128, 128)):\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.divide = divide\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.orig_image_size = (101, 101)\n",
    "        self.padding_pixels = None\n",
    "        \n",
    "        \"\"\"\n",
    "        root_path: folder specifying files location\n",
    "        file_list: list of images IDs\n",
    "        is_test: whether train or test data is used (contains masks or not)\n",
    "        \n",
    "        divide: whether to divide by 255\n",
    "        image_size: output image size, should be divisible by 32\n",
    "        \n",
    "        orig_image_size: original images size\n",
    "        padding_pixels: placeholder for list of padding dimensions\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "\n",
    "        file_id = self.file_list[index]\n",
    "\n",
    "        # Get image path\n",
    "        image_folder = os.path.join(self.root_path, 'images')\n",
    "        image_path = os.path.join(image_folder, file_id + '.png')\n",
    "    \n",
    "        # Get mask path\n",
    "        mask_folder = os.path.join(self.root_path, 'masks')\n",
    "        mask_path = os.path.join(mask_folder, file_id + '.png')\n",
    "\n",
    "        # Load image\n",
    "        image = self.__load_image(image_path)\n",
    "        if not self.is_test:\n",
    "            # Load mask for training or evaluation\n",
    "            mask = self.__load_image(mask_path, mask=True)\n",
    "            if self.divide:\n",
    "                image = image / 255.\n",
    "                mask = mask / 255.\n",
    "            # Transform into torch float Tensors of shape (CxHxW).\n",
    "            image = torch.from_numpy(\n",
    "                image).float().permute([2, 0, 1])\n",
    "            mask = torch.from_numpy(\n",
    "                np.expand_dims(mask, axis=-1)).float().permute([2, 0, 1])\n",
    "            return image, mask\n",
    "\n",
    "        if self.is_test:\n",
    "            if self.divide:\n",
    "                image = image / 255.\n",
    "            image = torch.from_numpy(image).float().permute([2, 0, 1])\n",
    "            return (image,)\n",
    "\n",
    "    def set_padding(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute padding borders for images based on original and specified image size.\n",
    "        \"\"\"\n",
    "        \n",
    "        pad_floor = np.floor(\n",
    "            (np.asarray(self.image_size) - np.asarray(self.orig_image_size)) / 2)\n",
    "        pad_ceil = np.ceil((np.asarray(self.image_size) -\n",
    "                            np.asarray(self.orig_image_size)) / 2)\n",
    "\n",
    "        self.padding_pixels = np.asarray(\n",
    "            (pad_floor[0], pad_ceil[0], pad_floor[1], pad_ceil[1])).astype(np.int32)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __pad_image(self, img):\n",
    "        \n",
    "        \"\"\"\n",
    "        Pad images according to border set in set_padding.\n",
    "        Original image is centered.\n",
    "        \"\"\"\n",
    "\n",
    "        y_min_pad, y_max_pad, x_min_pad, x_max_pad = self.padding_pixels[\n",
    "            0], self.padding_pixels[1], self.padding_pixels[2], self.padding_pixels[3]\n",
    "\n",
    "        img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad,\n",
    "                                 x_min_pad, x_max_pad,\n",
    "                                 cv2.BORDER_REPLICATE)\n",
    "\n",
    "        assert img.shape[:2] == self.image_size, '\\\n",
    "        Image after padding must have the same shape as input image.'\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __load_image(self, path, mask=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Helper function for loading image.\n",
    "        If mask is loaded, it is loaded in grayscale (, 0) parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        if mask:\n",
    "            img = cv2.imread(str(path), 0)\n",
    "        else:\n",
    "            img = cv2.imread(str(path), 0)\n",
    "\n",
    "        height, width = img.shape[0], img.shape[1]\n",
    "\n",
    "        img = self.__pad_image(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def return_padding_borders(self):\n",
    "        \"\"\"\n",
    "        Return padding borders to easily crop the images.\n",
    "        \"\"\"\n",
    "        return self.padding_pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load initial data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training images: 4000\n",
      "# of training masks: 4000\n",
      "# of test images: 18000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rle_mask</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>575d24d81d</th>\n",
       "      <td>NaN</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a266a2a9df</th>\n",
       "      <td>5051 5151</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75efad62c1</th>\n",
       "      <td>9 93 109 94 210 94 310 95 411 95 511 96 612 96...</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34e51dba6a</th>\n",
       "      <td>48 54 149 54 251 53 353 52 455 51 557 50 659 4...</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4875705fb0</th>\n",
       "      <td>1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     rle_mask    z\n",
       "id                                                                \n",
       "575d24d81d                                                NaN  843\n",
       "a266a2a9df                                          5051 5151  794\n",
       "75efad62c1  9 93 109 94 210 94 310 95 411 95 511 96 612 96...  468\n",
       "34e51dba6a  48 54 149 54 251 53 353 52 455 51 557 50 659 4...  727\n",
       "4875705fb0  1111 1 1212 1 1313 1 1414 1 1514 2 1615 2 1716...  797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col = 'id')\n",
    "depths_df = pd.read_csv('depths.csv', index_col='id')\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)] # All depths not in train dataset are in \n",
    "\n",
    "print ('# of training images:', len(os.listdir('train/images')))\n",
    "print ('# of training masks:', len(os.listdir('train/masks')))\n",
    "print ('# of test images:', len(os.listdir('test/images')))\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_src = ''\n",
    "\n",
    "quick_try = False\n",
    "grayscale = True\n",
    "\n",
    "orig_image_size = (101, 101)\n",
    "image_size = (128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7917ee2353e64b2cac14d708551808b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "print('Loading training set.')\n",
    "for i in tqdm_notebook(train_df.index):\n",
    "    img_src = 'train/images/{}.png'.format(i)\n",
    "    mask_src = 'train/masks/{}.png'.format(i)\n",
    "    if grayscale:\n",
    "        img_temp = cv2.imread(img_src, 0)\n",
    "    else:\n",
    "        img_temp = cv2.imread(img_src)\n",
    "    mask_temp = cv2.imread(mask_src, 0)\n",
    "    if orig_image_size != image_size:\n",
    "        img_temp = cv2.resize(img_temp, image_size)\n",
    "        mask_temp = cv2.resize(mask_temp, image_size)\n",
    "    X_train.append(img_temp)\n",
    "    y_train.append(mask_temp)\n",
    "    # print(img_temp.shape)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "if grayscale:\n",
    "    X_train = np.expand_dims(X_train, -1)\n",
    "y_train = np.expand_dims(y_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute mask coverage for each observation.\n"
     ]
    }
   ],
   "source": [
    "print('Compute mask coverage for each observation.')\n",
    "\n",
    "def cov_to_class(val):\n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i:\n",
    "            return i\n",
    "\n",
    "# Percent of area covered by mask.\n",
    "train_df['coverage'] = np.mean(y_train / 255., axis=(1, 2))\n",
    "train_df['coverage_class'] = train_df.coverage.map(\n",
    "    cov_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters for data loading:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train'\n",
    "test_path = 'test'\n",
    "\n",
    "train_ids = train_df.index.values\n",
    "test_ids = test_df.index.values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_ids, valid_ids, tr_coverage, valid_coverage = train_test_split(\n",
    "    train_ids,\n",
    "    train_df.coverage.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state= 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define Data Loading__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset:\n",
    "dataset_train = TGSSaltDataset(train_path, tr_ids, divide=True)\n",
    "dataset_train.set_padding()\n",
    "y_min_pad, y_max_pad, x_min_pad, x_max_pad = dataset_train.return_padding_borders()\n",
    "        \n",
    "# Validation dataset:\n",
    "dataset_val = TGSSaltDataset(train_path, valid_ids, divide=True)\n",
    "dataset_val.set_padding()\n",
    "\n",
    "# Test dataset:\n",
    "dataset_test = TGSSaltDataset(test_path, test_ids, is_test=True, divide=True)\n",
    "dataset_test.set_padding()\n",
    "\n",
    "\n",
    "# Data loaders:\n",
    "# Use multiple workers to optimize data loading speed.\n",
    "# Pin memory for quicker GPU processing.\n",
    "train_loader = data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)\n",
    "\n",
    "# Do not shuffle for validation and test.\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2995ae8cf564d48a1d8435067575453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-16-dd5bc7c6ab95>\", line 59, in __getitem__\n    image).float().permute([2, 0, 1])\nRuntimeError: number of dims don't match in permute\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3f07f5227e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Training:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Put image on chosen device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-16-dd5bc7c6ab95>\", line 59, in __getitem__\n    image).float().permute([2, 0, 1])\nRuntimeError: number of dims don't match in permute\n"
     ]
    }
   ],
   "source": [
    "# Get defined UNet model.\n",
    "model = resnet34()\n",
    "# Set Binary Crossentropy as loss function.\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Set optimizer.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train for n epochs\n",
    "n = 2\n",
    "for e in range(n):\n",
    "\n",
    "    # Training:\n",
    "    train_loss = []\n",
    "    for image, mask in tqdm_notebook(train_loader):\n",
    "\n",
    "        # Put image on chosen device\n",
    "        image = image.type(torch.float).to(device)\n",
    "        # Predict with model:\n",
    "        y_pred = model(image)\n",
    "        # Compute loss between true and predicted values\n",
    "        loss = loss_fn(y_pred, mask.to(device))\n",
    "\n",
    "        # Set model gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagate the loss.\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform single optimization step - parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append training loss\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    # Validation:\n",
    "    val_loss = []\n",
    "    val_iou = []\n",
    "    for image, mask in valid_loader:\n",
    "        \n",
    "        image = image.to(device)\n",
    "        y_pred = model(image)\n",
    "        \n",
    "        loss = loss_fn(y_pred, mask.to(device))\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "    print(\"Epoch: %d, Train: %.3f, Val: %.3f\" %\n",
    "          (e, np.mean(train_loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
